{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import os\n",
    "from librosa import display\n",
    "import librosa\n",
    "plt.rcParams['figure.figsize'] = 15, 8\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn as sk\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import InputLayer, Input\n",
    "from tensorflow.python.keras.layers import Reshape, MaxPooling2D\n",
    "from tensorflow.python.keras.layers import Conv2D, Dense, Flatten\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.models import load_model\n",
    "import skopt\n",
    "from skopt import gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt.plots import plot_objective, plot_evaluations\n",
    "from skopt.utils import use_named_args\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_features_data.p', 'rb') as f:\n",
    "    vgg_data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>label</th>\n",
       "      <th>file_name</th>\n",
       "      <th>fold</th>\n",
       "      <th>salience</th>\n",
       "      <th>class</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>...</th>\n",
       "      <th>886</th>\n",
       "      <th>887</th>\n",
       "      <th>888</th>\n",
       "      <th>889</th>\n",
       "      <th>890</th>\n",
       "      <th>891</th>\n",
       "      <th>892</th>\n",
       "      <th>893</th>\n",
       "      <th>894</th>\n",
       "      <th>895</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[157.5, 12.5, 163.5, 111.25, 182.25, 32.5, 89....</td>\n",
       "      <td>3</td>\n",
       "      <td>UrbanSound8K/audio/fold1/101415-3-0-2.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>157.5</td>\n",
       "      <td>12.5</td>\n",
       "      <td>163.5</td>\n",
       "      <td>111.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.922422</td>\n",
       "      <td>-0.502428</td>\n",
       "      <td>-0.341933</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.154701</td>\n",
       "      <td>-0.99787</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.054023</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 902 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            features label  \\\n",
       "0  [157.5, 12.5, 163.5, 111.25, 182.25, 32.5, 89....     3   \n",
       "\n",
       "                                   file_name fold salience     class      0  \\\n",
       "0  UrbanSound8K/audio/fold1/101415-3-0-2.wav    1        1  dog_bark  157.5   \n",
       "\n",
       "      1      2       3  ...  886       887       888       889  890       891  \\\n",
       "0  12.5  163.5  111.25  ...  0.0  0.922422 -0.502428 -0.341933  0.0  1.154701   \n",
       "\n",
       "       892  893       894  895  \n",
       "0 -0.99787  0.0 -0.054023  0.0  \n",
       "\n",
       "[1 rows x 902 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8275, 902)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_data['label']  = vgg_data['label'].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(vgg_data['features'])\n",
    "X = pd.DataFrame(X)\n",
    "data_cols = X.columns\n",
    "y=vgg_data['label']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8275, 896), (8275,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6692, 896), (1583, 896), (6692,), (1583,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = {}\n",
    "X_train = X[0:6692].values\n",
    "X_test = X[6692:].values\n",
    "y_train = y[0:6692].values\n",
    "y_test = y[6692:].values\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6692, 64, 14, 1), (1583, 64, 14, 1))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#reshaping to 2D \n",
    "X_train=np.reshape(X_train,(X_train.shape[0], 64,14,1))\n",
    "X_test=np.reshape(X_test,(X_test.shape[0], 64,14,1))\n",
    "X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, x_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "validation_data = (x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_flat = 896\n",
    "shape_full = (64, 14, 1)\n",
    "\n",
    "shape_data = (64, 14)\n",
    "\n",
    "num_channels = 1\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparamter Tuning with Sci-Kit Optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to optimize the Convolutional Neural Network in classifying the audio data accurately, we need to find the following hyper-parameters:\n",
    "\n",
    "- The learning-rate of the optimizer.\n",
    "- The number of fully-connected / dense layers.\n",
    "- The number of nodes for each of the dense layers.\n",
    "- Whether to use 'sigmoid' or 'relu' activation in all the layers.\n",
    "\n",
    "We will use the Python package scikit-optimize (or skopt) for finding the best choices of these hyper-parameters. Before we begin with the actual search for hyper-parameters, we first need to define the valid search-ranges or search-dimensions for each of these parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special thanks to Erik Hvass Pedersen for a detailed tutorial on Youtube and GitHub.\n",
    "\n",
    "The below helper functions and code has been borrowed from [Hvass-Labs](https://www.hvass-labs.org/) **>** TensorFlow-Tutorials **>** 19_Hyper-Parameters\n",
    "\n",
    "Please click the link for the video tutorial.\n",
    "\n",
    "<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=oaxf3rk0KGM\n",
    "\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/oaxf3rk0KGM/0.jpg\" \n",
    "alt=\"IMAGE ALT TEXT HERE\" width=\"240\" height=\"180\" border=\"10\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Search Dimensions\n",
    "\n",
    "This is the search-dimension for the learning-rate. It is a real number (floating-point) with a lower bound of 1e-6 and an upper bound of 1e-2. But instead of searching between these bounds directly, we use a logarithmic transformation, so we will search for the number k in 1ek which is only bounded between -6 and -2. This is better than searching the entire exponential range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_learning_rate = Real(low=1e-6, high=1e-2, prior='log-uniform',\n",
    "                         name='learning_rate')\n",
    "\n",
    "dim_num_dense_layers = Integer(low=1, high=3, name='num_dense_layers')\n",
    "\n",
    "dim_num_dense_nodes = Integer(low=16, high=512, name='num_dense_nodes')\n",
    "\n",
    "dim_activation = Categorical(categories=['relu', 'sigmoid'],\n",
    "                             name='activation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then combine all these search-dimensions into a list.\n",
    "dimensions = [dim_learning_rate,\n",
    "              dim_num_dense_layers,\n",
    "              dim_num_dense_nodes,\n",
    "              dim_activation]\n",
    "\n",
    "default_parameters = [1e-3, 1, 128, 'relu']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Helper-function for log-dir-name\n",
    "\n",
    "We will log the training-progress for all parameter-combinations so they can be viewed and compared using TensorBoard. This is done by setting a common parent-dir and then have a sub-dir for each parameter-combination with an appropriate name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_dir_name(learning_rate, num_dense_layers,\n",
    "                 num_dense_nodes, activation):\n",
    "\n",
    "    # The dir-name for the TensorBoard log-dir.\n",
    "    s = \"logs/lr_{0:.0e}_layers_{1}_nodes_{2}_{3}/\"\n",
    "\n",
    "    # Insert all the hyper-parameters in the dir-name.\n",
    "    log_dir = s.format(learning_rate,\n",
    "                       num_dense_layers,\n",
    "                       num_dense_nodes,\n",
    "                       activation)\n",
    "\n",
    "    return log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Helper-function for Create the Model\n",
    "\n",
    "We first need a function that takes a set of hyper-parameters and creates the Convolutional Neural Network corresponding to those parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(learning_rate, num_dense_layers,\n",
    "                 num_dense_nodes, activation):\n",
    "    \"\"\"\n",
    "    Hyper-parameters:\n",
    "    learning_rate:     Learning-rate for the optimizer.\n",
    "    num_dense_layers:  Number of dense layers.\n",
    "    num_dense_nodes:   Number of nodes in each dense layer.\n",
    "    activation:        Activation function for all layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start construction of a Keras Sequential model.\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add an input layer which is similar to a feed_dict in TensorFlow.\n",
    "    # Note that the input-shape must be a tuple containing the dim-size.\n",
    "    # model.add(InputLayer(input_shape=(size_flat,)))\n",
    "\n",
    "    # The input from VGG is a flattened array with 896 elements,\n",
    "    # but the convolutional layers expect VGG with shape (64, 14, 1)\n",
    "    # model.add(Reshape(shape_full))\n",
    "\n",
    "    # First convolutional layer. We are making this as an input layer\n",
    "    # Modified the layer as Input layer with shape (64, 14, 1) \n",
    "    # to avoid value valueError: \n",
    "    # You are trying to load a weight file containing 4 layers into a model with 0 layers site:stackoverflow.com\n",
    "    \n",
    "    model.add(Conv2D(kernel_size=5, strides=1, filters=16, padding='same',\n",
    "                     activation=activation, name='layer_conv1', input_shape=shape_full))\n",
    "    model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "    # Second convolutional layer.\n",
    "    # Again, we only want to optimize the activation-function here.\n",
    "    model.add(Conv2D(kernel_size=5, strides=1, filters=36, padding='same',\n",
    "                     activation=activation, name='layer_conv2'))\n",
    "    model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "    # Flatten the 4-rank output of the convolutional layers\n",
    "    # to 2-rank that can be input to a fully-connected / dense layer.\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Add fully-connected / dense layers.\n",
    "    # The number of layers is a hyper-parameter we want to optimize.\n",
    "    for i in range(num_dense_layers):\n",
    "        # Name of the layer. This is not really necessary\n",
    "        # because Keras should give them unique names.\n",
    "        name = 'layer_dense_{0}'.format(i+1)\n",
    "\n",
    "        # Add the dense / fully-connected layer to the model.\n",
    "        # This has two hyper-parameters we want to optimize:\n",
    "        # The number of nodes and the activation function.\n",
    "        model.add(Dense(num_dense_nodes,\n",
    "                        activation=activation,\n",
    "                        name=name))\n",
    "\n",
    "    # Last fully-connected / dense layer with softmax-activation\n",
    "    # for use in classification.\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # Use the Adam method for training the network.\n",
    "    # We want to find the best learning-rate for the Adam method.\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    # In Keras we need to compile the model so it can be trained.\n",
    "    # We will use Sparse Categorical Cross Entropy since we are not doing one-hot\n",
    "    # encoding on the \"class\"\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train and Evaluate the Model\n",
    "The neural network with the best hyper-parameters is saved to disk so it can be reloaded later. This is the filename for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_best_model = 'cnn_best_model.keras'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the classification accuracy for the model saved to disk. It is a global variable which will be updated during optimization of the hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the function that creates and trains a neural network with the given hyper-parameters, and then evaluates its performance on the validation-set. The function then returns the so-called fitness value (aka. objective value), which is the negative classification accuracy on the validation-set. It is negative because skopt performs minimization instead of maximization.\n",
    "\n",
    "Note the function decorator @use_named_args which wraps the fitness function so that it can be called with all the parameters as a single list, for example: fitness(x=[1e-4, 3, 256, 'relu']). This is the calling-style skopt uses internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness(learning_rate, num_dense_layers,\n",
    "            num_dense_nodes, activation):\n",
    "    \"\"\"\n",
    "    Hyper-parameters:\n",
    "    learning_rate:     Learning-rate for the optimizer.\n",
    "    num_dense_layers:  Number of dense layers.\n",
    "    num_dense_nodes:   Number of nodes in each dense layer.\n",
    "    activation:        Activation function for all layers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Print the hyper-parameters.\n",
    "    print('learning rate: {0:.1e}'.format(learning_rate))\n",
    "    print('num_dense_layers:', num_dense_layers)\n",
    "    print('num_dense_nodes:', num_dense_nodes)\n",
    "    print('activation:', activation)\n",
    "    print()\n",
    "    \n",
    "    # Create the neural network with these hyper-parameters.\n",
    "    model = create_model(learning_rate=learning_rate,\n",
    "                         num_dense_layers=num_dense_layers,\n",
    "                         num_dense_nodes=num_dense_nodes,\n",
    "                         activation=activation)\n",
    "\n",
    "    # Dir-name for the TensorBoard log-files.\n",
    "    log_dir = log_dir_name(learning_rate, num_dense_layers,\n",
    "                           num_dense_nodes, activation)\n",
    "    \n",
    "    # Create a callback-function for Keras which will be\n",
    "    # run after each epoch has ended during training.\n",
    "    # This saves the log-files for TensorBoard.\n",
    "    # Note that there are complications when histogram_freq=1.\n",
    "    # It might give strange errors and it also does not properly\n",
    "    # support Keras data-generators for the validation-set.\n",
    "    callback_log = TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=0,\n",
    "        batch_size=32,\n",
    "        write_graph=True,\n",
    "        write_grads=False,\n",
    "        write_images=False)\n",
    "   \n",
    "    # Use Keras to train the model.\n",
    "    history = model.fit(x=X_train,\n",
    "                        y=y_train,\n",
    "                        epochs=5,  \n",
    "                        batch_size=128,\n",
    "                        validation_data=validation_data,\n",
    "                        callbacks=[callback_log])\n",
    "\n",
    "    # Get the classification accuracy on the validation-set\n",
    "    # after the last training-epoch.\n",
    "    accuracy = history.history['val_acc'][-1]\n",
    "\n",
    "    # Print the classification accuracy.\n",
    "    print()\n",
    "    print(\"Accuracy: {0:.2%}\".format(accuracy))\n",
    "    print()\n",
    "\n",
    "    # Save the model if it improves on the best-found performance.\n",
    "    # We use the global keyword so we update the variable outside\n",
    "    # of this function.\n",
    "    global best_accuracy\n",
    "\n",
    "    # If the classification accuracy of the saved model is improved ...\n",
    "    if accuracy > best_accuracy:\n",
    "        # Save the new model to harddisk.\n",
    "        model.save(path_best_model)\n",
    "       \n",
    "        # Update the classification accuracy.\n",
    "        best_accuracy = accuracy\n",
    "\n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "    \n",
    "    # Clear the Keras session, otherwise it will keep adding new\n",
    "    # models to the same TensorFlow graph each time we create\n",
    "    # a model with a different set of hyper-parameters.\n",
    "    K.clear_session()\n",
    "    \n",
    "    # NOTE: Scikit-optimize does minimization so it tries to\n",
    "    # find a set of hyper-parameters with the LOWEST fitness-value.\n",
    "    # Because we are interested in the HIGHEST classification\n",
    "    # accuracy, we need to negate this number so it can be minimized.\n",
    "    return -accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate: 1.0e-03\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 128\n",
      "activation: relu\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\rkalidindi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Train on 6022 samples, validate on 670 samples\n",
      "Epoch 1/5\n",
      "6022/6022 [==============================] - 7s 1ms/sample - loss: 1.4479 - acc: 0.5214 - val_loss: 1.0445 - val_acc: 0.6731\n",
      "Epoch 2/5\n",
      "6022/6022 [==============================] - 6s 976us/sample - loss: 0.7461 - acc: 0.7554 - val_loss: 0.7444 - val_acc: 0.7627\n",
      "Epoch 3/5\n",
      "6022/6022 [==============================] - 5s 906us/sample - loss: 0.5501 - acc: 0.8250 - val_loss: 0.6130 - val_acc: 0.8075\n",
      "Epoch 4/5\n",
      "6022/6022 [==============================] - 6s 960us/sample - loss: 0.4286 - acc: 0.8637 - val_loss: 0.8160 - val_acc: 0.7493\n",
      "Epoch 5/5\n",
      "6022/6022 [==============================] - 6s 944us/sample - loss: 0.3788 - acc: 0.8804 - val_loss: 0.5077 - val_acc: 0.8478\n",
      "\n",
      "Accuracy: 84.78%\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.8477612"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Run\n",
    "# Before we run the hyper-parameter optimization, let us first check that the various functions \n",
    "# above actually work, when we pass the default hyper-parameters.\n",
    "fitness(x=default_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Test run with default parameters (1e-3, 1, 128, 'relu') gave an accuracy of 84.78%*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the Hyper-Parameter Optimization\n",
    "\n",
    "Now we are ready to run the actual hyper-parameter optimization using Bayesian optimization from the scikit-optimize package. Note that it first calls fitness() with default_parameters as the starting point we have found by hand-tuning, which should help the optimizer locate better hyper-parameters faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate: 1.0e-03\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 128\n",
      "activation: relu\n",
      "\n",
      "Train on 6022 samples, validate on 670 samples\n",
      "Epoch 1/5\n",
      "6022/6022 [==============================] - 6s 1ms/sample - loss: 1.4546 - acc: 0.5188 - val_loss: 0.9355 - val_acc: 0.7090\n",
      "Epoch 2/5\n",
      "6022/6022 [==============================] - 7s 1ms/sample - loss: 0.7367 - acc: 0.7645 - val_loss: 0.8398 - val_acc: 0.7060\n",
      "Epoch 3/5\n",
      "6022/6022 [==============================] - 6s 1ms/sample - loss: 0.5413 - acc: 0.8268 - val_loss: 0.6675 - val_acc: 0.7881\n",
      "Epoch 4/5\n",
      "6022/6022 [==============================] - 6s 931us/sample - loss: 0.4366 - acc: 0.8584 - val_loss: 0.5068 - val_acc: 0.8358\n",
      "Epoch 5/5\n",
      "6022/6022 [==============================] - 6s 950us/sample - loss: 0.3210 - acc: 0.9030 - val_loss: 0.5552 - val_acc: 0.8254\n",
      "\n",
      "Accuracy: 82.54%\n",
      "\n",
      "learning rate: 2.3e-04\n",
      "num_dense_layers: 2\n",
      "num_dense_nodes: 250\n",
      "activation: relu\n",
      "\n",
      "Train on 6022 samples, validate on 670 samples\n",
      "Epoch 1/5\n",
      "6022/6022 [==============================] - 7s 1ms/sample - loss: 1.9052 - acc: 0.4005 - val_loss: 1.5014 - val_acc: 0.5119\n",
      "Epoch 2/5\n",
      "6022/6022 [==============================] - 6s 1ms/sample - loss: 1.2172 - acc: 0.6496 - val_loss: 1.0521 - val_acc: 0.6642\n",
      "Epoch 3/5\n",
      "6022/6022 [==============================] - 6s 958us/sample - loss: 0.8997 - acc: 0.7199 - val_loss: 0.9345 - val_acc: 0.7000\n",
      "Epoch 4/5\n",
      "6022/6022 [==============================] - 6s 963us/sample - loss: 0.7628 - acc: 0.7627 - val_loss: 0.7865 - val_acc: 0.7284\n",
      "Epoch 5/5\n",
      "6022/6022 [==============================] - 6s 978us/sample - loss: 0.6477 - acc: 0.7962 - val_loss: 0.6908 - val_acc: 0.7701\n",
      "\n",
      "Accuracy: 77.01%\n",
      "\n",
      "learning rate: 1.8e-05\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 33\n",
      "activation: sigmoid\n",
      "\n",
      "Train on 6022 samples, validate on 670 samples\n",
      "Epoch 1/5\n",
      "6022/6022 [==============================] - 6s 930us/sample - loss: 2.5053 - acc: 0.1016 - val_loss: 2.5029 - val_acc: 0.1164\n",
      "Epoch 2/5\n",
      "6022/6022 [==============================] - 6s 988us/sample - loss: 2.4829 - acc: 0.1016 - val_loss: 2.4821 - val_acc: 0.1164\n",
      "Epoch 3/5\n",
      "6022/6022 [==============================] - 6s 957us/sample - loss: 2.4646 - acc: 0.1016 - val_loss: 2.4655 - val_acc: 0.1164\n",
      "Epoch 4/5\n",
      "6022/6022 [==============================] - 6s 964us/sample - loss: 2.4497 - acc: 0.1016 - val_loss: 2.4516 - val_acc: 0.1164\n",
      "Epoch 5/5\n",
      "6022/6022 [==============================] - 6s 976us/sample - loss: 2.4372 - acc: 0.1016 - val_loss: 2.4393 - val_acc: 0.1164\n",
      "\n",
      "Accuracy: 11.64%\n",
      "\n",
      "learning rate: 7.4e-04\n",
      "num_dense_layers: 2\n",
      "num_dense_nodes: 452\n",
      "activation: relu\n",
      "\n",
      "Train on 6022 samples, validate on 670 samples\n",
      "Epoch 1/5\n",
      "6022/6022 [==============================] - 7s 1ms/sample - loss: 1.3999 - acc: 0.5269 - val_loss: 0.9910 - val_acc: 0.6731\n",
      "Epoch 2/5\n",
      "6022/6022 [==============================] - 7s 1ms/sample - loss: 0.6592 - acc: 0.7888 - val_loss: 0.7365 - val_acc: 0.7612\n",
      "Epoch 3/5\n",
      "6022/6022 [==============================] - 7s 1ms/sample - loss: 0.4879 - acc: 0.8403 - val_loss: 0.6659 - val_acc: 0.7940\n",
      "Epoch 4/5\n",
      "6022/6022 [==============================] - 7s 1ms/sample - loss: 0.3432 - acc: 0.8889 - val_loss: 0.4865 - val_acc: 0.8418\n",
      "Epoch 5/5\n",
      "6022/6022 [==============================] - 7s 1ms/sample - loss: 0.2579 - acc: 0.9185 - val_loss: 0.4898 - val_acc: 0.8418\n",
      "\n",
      "Accuracy: 84.18%\n",
      "\n",
      "learning rate: 3.7e-03\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 475\n",
      "activation: sigmoid\n",
      "\n",
      "Train on 6022 samples, validate on 670 samples\n",
      "Epoch 1/5\n",
      "6022/6022 [==============================] - 7s 1ms/sample - loss: 2.5513 - acc: 0.1134 - val_loss: 2.2702 - val_acc: 0.1179\n",
      "Epoch 2/5\n",
      "6022/6022 [==============================] - 7s 1ms/sample - loss: 2.1187 - acc: 0.2150 - val_loss: 1.9558 - val_acc: 0.2582\n",
      "Epoch 3/5\n",
      "6022/6022 [==============================] - 8s 1ms/sample - loss: 1.6032 - acc: 0.4507 - val_loss: 1.4350 - val_acc: 0.5134\n",
      "Epoch 4/5\n",
      "6022/6022 [==============================] - 7s 1ms/sample - loss: 1.1323 - acc: 0.6211 - val_loss: 1.0737 - val_acc: 0.6373\n",
      "Epoch 5/5\n",
      "6022/6022 [==============================] - 8s 1ms/sample - loss: 0.9490 - acc: 0.6769 - val_loss: 0.9815 - val_acc: 0.6657\n",
      "\n",
      "Accuracy: 66.57%\n",
      "\n",
      "learning rate: 9.4e-05\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 291\n",
      "activation: sigmoid\n",
      "\n",
      "Train on 6022 samples, validate on 670 samples\n",
      "Epoch 1/5\n",
      "6022/6022 [==============================] - 9s 2ms/sample - loss: 2.3335 - acc: 0.1152 - val_loss: 2.2236 - val_acc: 0.1090\n",
      "Epoch 2/5\n",
      "6022/6022 [==============================] - 9s 1ms/sample - loss: 2.2427 - acc: 0.1226 - val_loss: 2.2199 - val_acc: 0.1269\n",
      "Epoch 3/5\n",
      "6022/6022 [==============================] - 11s 2ms/sample - loss: 2.2442 - acc: 0.1186 - val_loss: 2.2251 - val_acc: 0.1090\n",
      "Epoch 4/5\n",
      "6022/6022 [==============================] - 11s 2ms/sample - loss: 2.2424 - acc: 0.1159 - val_loss: 2.2249 - val_acc: 0.1090\n",
      "Epoch 5/5\n",
      "6022/6022 [==============================] - 11s 2ms/sample - loss: 2.2424 - acc: 0.1169 - val_loss: 2.2217 - val_acc: 0.1194\n",
      "\n",
      "Accuracy: 11.94%\n",
      "\n",
      "learning rate: 2.0e-04\n",
      "num_dense_layers: 2\n",
      "num_dense_nodes: 185\n",
      "activation: relu\n",
      "\n",
      "Train on 6022 samples, validate on 670 samples\n",
      "Epoch 1/5\n",
      "6022/6022 [==============================] - 10s 2ms/sample - loss: 1.9457 - acc: 0.3507 - val_loss: 1.5851 - val_acc: 0.4866\n",
      "Epoch 2/5\n",
      "6022/6022 [==============================] - 11s 2ms/sample - loss: 1.2974 - acc: 0.6219 - val_loss: 1.1642 - val_acc: 0.6343\n",
      "Epoch 3/5\n",
      "6022/6022 [==============================] - 11s 2ms/sample - loss: 0.9799 - acc: 0.6996 - val_loss: 0.9712 - val_acc: 0.6746\n",
      "Epoch 4/5\n",
      "6022/6022 [==============================] - 10s 2ms/sample - loss: 0.8284 - acc: 0.7373 - val_loss: 0.8601 - val_acc: 0.7299\n",
      "Epoch 5/5\n",
      "6022/6022 [==============================] - 10s 2ms/sample - loss: 0.7440 - acc: 0.7595 - val_loss: 0.7872 - val_acc: 0.7403\n",
      "\n",
      "Accuracy: 74.03%\n",
      "\n",
      "learning rate: 1.2e-05\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 278\n",
      "activation: sigmoid\n",
      "\n",
      "Train on 6022 samples, validate on 670 samples\n",
      "Epoch 1/5\n",
      "6022/6022 [==============================] - 18s 3ms/sample - loss: 2.6443 - acc: 0.1196 - val_loss: 2.5357 - val_acc: 0.1194\n",
      "Epoch 2/5\n",
      "6022/6022 [==============================] - 12s 2ms/sample - loss: 2.4985 - acc: 0.1196 - val_loss: 2.4190 - val_acc: 0.1194\n",
      "Epoch 3/5\n",
      "6022/6022 [==============================] - 9s 2ms/sample - loss: 2.4005 - acc: 0.1196 - val_loss: 2.3436 - val_acc: 0.1194\n",
      "Epoch 4/5\n",
      "6022/6022 [==============================] - 9s 1ms/sample - loss: 2.3377 - acc: 0.1196 - val_loss: 2.2958 - val_acc: 0.1194\n",
      "Epoch 5/5\n",
      "6022/6022 [==============================] - 9s 1ms/sample - loss: 2.2969 - acc: 0.1196 - val_loss: 2.2668 - val_acc: 0.1194\n",
      "\n",
      "Accuracy: 11.94%\n",
      "\n",
      "learning rate: 3.3e-06\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 299\n",
      "activation: relu\n",
      "\n",
      "Train on 6022 samples, validate on 670 samples\n",
      "Epoch 1/5\n",
      "6022/6022 [==============================] - 10s 2ms/sample - loss: 2.2980 - acc: 0.1159 - val_loss: 2.2710 - val_acc: 0.1328\n",
      "Epoch 2/5\n",
      "6022/6022 [==============================] - 11s 2ms/sample - loss: 2.2353 - acc: 0.1485 - val_loss: 2.2224 - val_acc: 0.1522\n",
      "Epoch 3/5\n",
      "6022/6022 [==============================] - 10s 2ms/sample - loss: 2.1929 - acc: 0.2101 - val_loss: 2.1854 - val_acc: 0.1970\n",
      "Epoch 4/5\n",
      "6022/6022 [==============================] - 9s 2ms/sample - loss: 2.1599 - acc: 0.2380 - val_loss: 2.1559 - val_acc: 0.2239\n",
      "Epoch 5/5\n",
      "6022/6022 [==============================] - 11s 2ms/sample - loss: 2.1325 - acc: 0.2587 - val_loss: 2.1302 - val_acc: 0.2463\n",
      "\n",
      "Accuracy: 24.63%\n",
      "\n",
      "learning rate: 2.2e-04\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 147\n",
      "activation: sigmoid\n",
      "\n",
      "Train on 6022 samples, validate on 670 samples\n",
      "Epoch 1/5\n",
      "6022/6022 [==============================] - 18s 3ms/sample - loss: 2.2789 - acc: 0.1126 - val_loss: 2.2273 - val_acc: 0.0970\n",
      "Epoch 2/5\n",
      "6022/6022 [==============================] - 14s 2ms/sample - loss: 2.2441 - acc: 0.1277 - val_loss: 2.2180 - val_acc: 0.1403\n",
      "Epoch 3/5\n",
      "6022/6022 [==============================] - 17s 3ms/sample - loss: 2.2462 - acc: 0.1277 - val_loss: 2.2198 - val_acc: 0.1403\n",
      "Epoch 4/5\n",
      "6022/6022 [==============================] - 13s 2ms/sample - loss: 2.2400 - acc: 0.1307 - val_loss: 2.2181 - val_acc: 0.1090\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6022/6022 [==============================] - 9s 1ms/sample - loss: 2.2277 - acc: 0.1314 - val_loss: 2.1998 - val_acc: 0.1836\n",
      "\n",
      "Accuracy: 18.36%\n",
      "\n",
      "learning rate: 9.4e-06\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 434\n",
      "activation: relu\n",
      "\n",
      "Train on 6022 samples, validate on 670 samples\n",
      "Epoch 1/5\n",
      "6022/6022 [==============================] - 11s 2ms/sample - loss: 2.3189 - acc: 0.1428 - val_loss: 2.2248 - val_acc: 0.2179\n",
      "Epoch 2/5\n",
      "6022/6022 [==============================] - 11s 2ms/sample - loss: 2.1680 - acc: 0.2496 - val_loss: 2.1274 - val_acc: 0.2716\n",
      "Epoch 3/5\n",
      "6022/6022 [==============================] - 9s 2ms/sample - loss: 2.0820 - acc: 0.3296 - val_loss: 2.0500 - val_acc: 0.3358\n",
      "Epoch 4/5\n",
      "6022/6022 [==============================] - 9s 2ms/sample - loss: 2.0057 - acc: 0.4004 - val_loss: 1.9771 - val_acc: 0.3910\n",
      "Epoch 5/5\n",
      "6022/6022 [==============================] - 9s 2ms/sample - loss: 1.9287 - acc: 0.4406 - val_loss: 1.9038 - val_acc: 0.4552\n",
      "\n",
      "Accuracy: 45.52%\n",
      "\n",
      "learning rate: 1.0e-02\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 512\n",
      "activation: relu\n",
      "\n",
      "Train on 6022 samples, validate on 670 samples\n",
      "Epoch 1/5\n",
      "6022/6022 [==============================] - 15s 2ms/sample - loss: 2.9330 - acc: 0.1139 - val_loss: 2.2308 - val_acc: 0.1269\n",
      "Epoch 2/5\n",
      "6022/6022 [==============================] - 17s 3ms/sample - loss: 2.2450 - acc: 0.1133 - val_loss: 2.2219 - val_acc: 0.1403\n",
      "Epoch 3/5\n",
      "6022/6022 [==============================] - 13s 2ms/sample - loss: 2.2425 - acc: 0.1159 - val_loss: 2.2262 - val_acc: 0.1090\n",
      "Epoch 4/5\n",
      "6022/6022 [==============================] - 15s 2ms/sample - loss: 2.2437 - acc: 0.1202 - val_loss: 2.2284 - val_acc: 0.0970\n",
      "Epoch 5/5\n",
      "6022/6022 [==============================] - 9s 1ms/sample - loss: 2.2417 - acc: 0.1199 - val_loss: 2.2183 - val_acc: 0.1403\n",
      "\n",
      "Accuracy: 14.03%\n",
      "\n",
      "learning rate: 1.0e-02\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 16\n",
      "activation: sigmoid\n",
      "\n",
      "Train on 6022 samples, validate on 670 samples\n",
      "Epoch 1/5\n",
      "6022/6022 [==============================] - 9s 1ms/sample - loss: 2.2679 - acc: 0.1267 - val_loss: 2.2214 - val_acc: 0.1194\n",
      "Epoch 2/5\n",
      "6022/6022 [==============================] - 7s 1ms/sample - loss: 2.2473 - acc: 0.1124 - val_loss: 2.2232 - val_acc: 0.1090\n",
      "Epoch 3/5\n",
      "6022/6022 [==============================] - 7s 1ms/sample - loss: 2.2437 - acc: 0.1164 - val_loss: 2.2246 - val_acc: 0.1194\n",
      "Epoch 4/5\n",
      "6022/6022 [==============================] - 7s 1ms/sample - loss: 2.2423 - acc: 0.1194 - val_loss: 2.2359 - val_acc: 0.0970\n",
      "Epoch 5/5\n",
      "6022/6022 [==============================] - 8s 1ms/sample - loss: 2.2445 - acc: 0.1181 - val_loss: 2.2225 - val_acc: 0.1194\n",
      "\n",
      "Accuracy: 11.94%\n",
      "\n",
      "learning rate: 1.0e-03\n",
      "num_dense_layers: 2\n",
      "num_dense_nodes: 425\n",
      "activation: relu\n",
      "\n",
      "Train on 6022 samples, validate on 670 samples\n",
      "Epoch 1/5\n",
      "6022/6022 [==============================] - 10s 2ms/sample - loss: 1.4308 - acc: 0.4968 - val_loss: 0.9882 - val_acc: 0.6731\n",
      "Epoch 2/5\n",
      "6022/6022 [==============================] - 11s 2ms/sample - loss: 0.6721 - acc: 0.7740 - val_loss: 0.6348 - val_acc: 0.7925\n",
      "Epoch 3/5\n",
      "6022/6022 [==============================] - 12s 2ms/sample - loss: 0.4432 - acc: 0.8565 - val_loss: 0.5773 - val_acc: 0.8134\n",
      "Epoch 4/5\n",
      "6022/6022 [==============================] - 14s 2ms/sample - loss: 0.3284 - acc: 0.8914 - val_loss: 0.5027 - val_acc: 0.8299\n",
      "Epoch 5/5\n",
      "6022/6022 [==============================] - 11s 2ms/sample - loss: 0.2101 - acc: 0.9342 - val_loss: 0.3839 - val_acc: 0.8836\n",
      "\n",
      "Accuracy: 88.36%\n",
      "\n",
      "learning rate: 8.9e-03\n",
      "num_dense_layers: 2\n",
      "num_dense_nodes: 484\n",
      "activation: relu\n",
      "\n",
      "Train on 6022 samples, validate on 670 samples\n",
      "Epoch 1/5\n",
      "6022/6022 [==============================] - 10s 2ms/sample - loss: 13.9754 - acc: 0.1167 - val_loss: 14.0733 - val_acc: 0.1269\n",
      "Epoch 2/5\n",
      "6022/6022 [==============================] - 9s 1ms/sample - loss: 14.2044 - acc: 0.1187 - val_loss: 14.0733 - val_acc: 0.1269\n",
      "Epoch 3/5\n",
      "6022/6022 [==============================] - 9s 1ms/sample - loss: 14.2044 - acc: 0.1187 - val_loss: 14.0733 - val_acc: 0.1269\n",
      "Epoch 4/5\n",
      "6022/6022 [==============================] - 9s 1ms/sample - loss: 14.2044 - acc: 0.1187 - val_loss: 14.0733 - val_acc: 0.1269\n",
      "Epoch 5/5\n",
      "6022/6022 [==============================] - 9s 1ms/sample - loss: 14.2044 - acc: 0.1187 - val_loss: 14.0733 - val_acc: 0.1269\n",
      "\n",
      "Accuracy: 12.69%\n",
      "\n",
      "learning rate: 1.3e-03\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 512\n",
      "activation: relu\n",
      "\n",
      "Train on 6022 samples, validate on 670 samples\n",
      "Epoch 1/5\n",
      "6022/6022 [==============================] - 10s 2ms/sample - loss: 1.5106 - acc: 0.4691 - val_loss: 1.1240 - val_acc: 0.6015\n",
      "Epoch 2/5\n",
      "6022/6022 [==============================] - 10s 2ms/sample - loss: 0.7125 - acc: 0.7521 - val_loss: 0.6469 - val_acc: 0.7806\n",
      "Epoch 3/5\n",
      "6022/6022 [==============================] - 9s 2ms/sample - loss: 0.4553 - acc: 0.8519 - val_loss: 0.5864 - val_acc: 0.8090\n",
      "Epoch 4/5\n",
      "6022/6022 [==============================] - 11s 2ms/sample - loss: 0.3897 - acc: 0.8710 - val_loss: 0.6500 - val_acc: 0.7940\n",
      "Epoch 5/5\n",
      "6022/6022 [==============================] - 12s 2ms/sample - loss: 0.2477 - acc: 0.9166 - val_loss: 0.5415 - val_acc: 0.8224\n",
      "\n",
      "Accuracy: 82.24%\n",
      "\n",
      "learning rate: 1.5e-03\n",
      "num_dense_layers: 1\n",
      "num_dense_nodes: 512\n",
      "activation: sigmoid\n",
      "\n",
      "Train on 6022 samples, validate on 670 samples\n",
      "Epoch 1/5\n",
      "6022/6022 [==============================] - 11s 2ms/sample - loss: 2.3727 - acc: 0.1146 - val_loss: 2.2543 - val_acc: 0.1090\n",
      "Epoch 2/5\n",
      "6022/6022 [==============================] - 11s 2ms/sample - loss: 2.1872 - acc: 0.1777 - val_loss: 2.0543 - val_acc: 0.1985\n",
      "Epoch 3/5\n",
      "6022/6022 [==============================] - 10s 2ms/sample - loss: 1.9156 - acc: 0.2961 - val_loss: 1.7725 - val_acc: 0.3687\n",
      "Epoch 4/5\n",
      "6022/6022 [==============================] - 10s 2ms/sample - loss: 1.5054 - acc: 0.4877 - val_loss: 1.3680 - val_acc: 0.5403\n",
      "Epoch 5/5\n",
      "6022/6022 [==============================] - 9s 1ms/sample - loss: 1.1894 - acc: 0.6031 - val_loss: 1.2470 - val_acc: 0.5761\n",
      "\n",
      "Accuracy: 57.61%\n",
      "\n",
      "learning rate: 4.8e-05\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 512\n",
      "activation: relu\n",
      "\n",
      "Train on 6022 samples, validate on 670 samples\n",
      "Epoch 1/5\n",
      "6022/6022 [==============================] - 10s 2ms/sample - loss: 2.1173 - acc: 0.2861 - val_loss: 1.9234 - val_acc: 0.4493\n",
      "Epoch 2/5\n",
      "6022/6022 [==============================] - 9s 1ms/sample - loss: 1.7270 - acc: 0.4849 - val_loss: 1.5298 - val_acc: 0.5075\n",
      "Epoch 3/5\n",
      "6022/6022 [==============================] - 9s 1ms/sample - loss: 1.3454 - acc: 0.6211 - val_loss: 1.2461 - val_acc: 0.6239\n",
      "Epoch 4/5\n",
      "6022/6022 [==============================] - 9s 1ms/sample - loss: 1.1013 - acc: 0.6775 - val_loss: 1.0716 - val_acc: 0.6910\n",
      "Epoch 5/5\n",
      "6022/6022 [==============================] - 10s 2ms/sample - loss: 0.9721 - acc: 0.7084 - val_loss: 0.9803 - val_acc: 0.7000\n",
      "\n",
      "Accuracy: 70.00%\n",
      "\n",
      "learning rate: 1.5e-04\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 512\n",
      "activation: relu\n",
      "\n",
      "Train on 6022 samples, validate on 670 samples\n",
      "Epoch 1/5\n",
      "6022/6022 [==============================] - 12s 2ms/sample - loss: 1.8447 - acc: 0.3741 - val_loss: 1.4693 - val_acc: 0.5000\n",
      "Epoch 2/5\n",
      "6022/6022 [==============================] - 11s 2ms/sample - loss: 1.1681 - acc: 0.6325 - val_loss: 1.0841 - val_acc: 0.6313\n",
      "Epoch 3/5\n",
      "6022/6022 [==============================] - 11s 2ms/sample - loss: 0.9112 - acc: 0.7026 - val_loss: 0.9355 - val_acc: 0.6925\n",
      "Epoch 4/5\n",
      "6022/6022 [==============================] - 12s 2ms/sample - loss: 0.7575 - acc: 0.7494 - val_loss: 0.8703 - val_acc: 0.7373\n",
      "Epoch 5/5\n",
      "6022/6022 [==============================] - 11s 2ms/sample - loss: 0.6619 - acc: 0.7821 - val_loss: 0.7151 - val_acc: 0.7552\n",
      "\n",
      "Accuracy: 75.52%\n",
      "\n",
      "learning rate: 1.0e-06\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 16\n",
      "activation: sigmoid\n",
      "\n",
      "Train on 6022 samples, validate on 670 samples\n",
      "Epoch 1/5\n",
      "6022/6022 [==============================] - 8s 1ms/sample - loss: 2.3997 - acc: 0.0395 - val_loss: 2.4129 - val_acc: 0.0269\n",
      "Epoch 2/5\n",
      "6022/6022 [==============================] - 7s 1ms/sample - loss: 2.3992 - acc: 0.0395 - val_loss: 2.4124 - val_acc: 0.0269\n",
      "Epoch 3/5\n",
      "6022/6022 [==============================] - 8s 1ms/sample - loss: 2.3988 - acc: 0.0395 - val_loss: 2.4119 - val_acc: 0.0269\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6022/6022 [==============================] - 8s 1ms/sample - loss: 2.3983 - acc: 0.0395 - val_loss: 2.4113 - val_acc: 0.0269\n",
      "Epoch 5/5\n",
      "6022/6022 [==============================] - 8s 1ms/sample - loss: 2.3979 - acc: 0.0395 - val_loss: 2.4108 - val_acc: 0.0269\n",
      "\n",
      "Accuracy: 2.69%\n",
      "\n",
      "learning rate: 5.1e-04\n",
      "num_dense_layers: 2\n",
      "num_dense_nodes: 512\n",
      "activation: relu\n",
      "\n",
      "Train on 6022 samples, validate on 670 samples\n",
      "Epoch 1/5\n",
      "6022/6022 [==============================] - 10s 2ms/sample - loss: 1.5055 - acc: 0.4867 - val_loss: 1.1168 - val_acc: 0.6403\n",
      "Epoch 2/5\n",
      "6022/6022 [==============================] - 9s 1ms/sample - loss: 0.7876 - acc: 0.7506 - val_loss: 0.7402 - val_acc: 0.7657\n",
      "Epoch 3/5\n",
      "6022/6022 [==============================] - 10s 2ms/sample - loss: 0.5589 - acc: 0.8251 - val_loss: 0.6207 - val_acc: 0.7896\n",
      "Epoch 4/5\n",
      "6022/6022 [==============================] - 12s 2ms/sample - loss: 0.4024 - acc: 0.8771 - val_loss: 0.7526 - val_acc: 0.7537\n",
      "Epoch 5/5\n",
      "6022/6022 [==============================] - 13s 2ms/sample - loss: 0.3357 - acc: 0.8972 - val_loss: 0.4587 - val_acc: 0.8627\n",
      "\n",
      "Accuracy: 86.27%\n",
      "\n",
      "learning rate: 7.9e-04\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 512\n",
      "activation: relu\n",
      "\n",
      "Train on 6022 samples, validate on 670 samples\n",
      "Epoch 1/5\n",
      "6022/6022 [==============================] - 13s 2ms/sample - loss: 1.4239 - acc: 0.5012 - val_loss: 0.9661 - val_acc: 0.6642\n",
      "Epoch 2/5\n",
      "6022/6022 [==============================] - 11s 2ms/sample - loss: 0.6687 - acc: 0.7720 - val_loss: 0.7694 - val_acc: 0.7388\n",
      "Epoch 3/5\n",
      "6022/6022 [==============================] - 9s 1ms/sample - loss: 0.4183 - acc: 0.8623 - val_loss: 0.5080 - val_acc: 0.8328\n",
      "Epoch 4/5\n",
      "6022/6022 [==============================] - 9s 1ms/sample - loss: 0.2747 - acc: 0.9156 - val_loss: 0.5868 - val_acc: 0.8149\n",
      "Epoch 5/5\n",
      "6022/6022 [==============================] - 9s 2ms/sample - loss: 0.2015 - acc: 0.9346 - val_loss: 0.4302 - val_acc: 0.8642\n",
      "\n",
      "Accuracy: 86.42%\n",
      "\n",
      "learning rate: 5.8e-04\n",
      "num_dense_layers: 3\n",
      "num_dense_nodes: 16\n",
      "activation: relu\n",
      "\n",
      "Train on 6022 samples, validate on 670 samples\n",
      "Epoch 1/5\n",
      "6022/6022 [==============================] - 9s 2ms/sample - loss: 2.0692 - acc: 0.2368 - val_loss: 1.7965 - val_acc: 0.3493\n",
      "Epoch 2/5\n",
      "6022/6022 [==============================] - 8s 1ms/sample - loss: 1.5551 - acc: 0.4894 - val_loss: 1.4336 - val_acc: 0.4866\n",
      "Epoch 3/5\n",
      "6022/6022 [==============================] - 8s 1ms/sample - loss: 1.2278 - acc: 0.5807 - val_loss: 1.2987 - val_acc: 0.5254\n",
      "Epoch 4/5\n",
      "6022/6022 [==============================] - 8s 1ms/sample - loss: 1.0492 - acc: 0.6503 - val_loss: 1.0527 - val_acc: 0.6612\n",
      "Epoch 5/5\n",
      "6022/6022 [==============================] - 9s 2ms/sample - loss: 0.8887 - acc: 0.7149 - val_loss: 0.8951 - val_acc: 0.7149\n",
      "\n",
      "Accuracy: 71.49%\n",
      "\n",
      "learning rate: 3.6e-04\n",
      "num_dense_layers: 2\n",
      "num_dense_nodes: 512\n",
      "activation: relu\n",
      "\n",
      "Train on 6022 samples, validate on 670 samples\n",
      "Epoch 1/5\n",
      "2176/6022 [=========>....................] - ETA: 9s - loss: 2.0037 - acc: 0.3382"
     ]
    }
   ],
   "source": [
    "# Run the Hyper-Parameter Optimization\n",
    "search_result = gp_minimize(func=fitness,\n",
    "                            dimensions=dimensions,\n",
    "                            acq_func='EI', # Expected Improvement.\n",
    "                            n_calls=40,\n",
    "                            x0=default_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Optimization Progress\n",
    "plot_convergence(search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Hyper-Parameters - learning_rate, num_dense_layers, num_dense_nodes, activation\n",
    "search_result.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the fitness value associated with these hyper-parameters. This is a negative number because the Bayesian optimizer performs minimization, so we had to negate the classification accuracy which is posed as a maximization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_result.fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see all the hyper-parameters tried by the Bayesian optimizer and their associated fitness values (the negated classification accuracies). These are sorted so the highest classification accuracies are shown first.\n",
    "\n",
    "It appears that 'relu' activation was generally better than 'sigmoid'. Otherwise it can be difficult to see a pattern of which parameter choices are good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(zip(search_result.func_vals, search_result.x_iters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate Best Model on Test-Set\n",
    "\n",
    "We can now use the best model on the test-set. It is very easy to reload the model using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(path_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(x=X_test,\n",
    "                        y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, value in zip(model.metrics_names, result):\n",
    "    print(name, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{0}: {1:.2%}\".format(model.metrics_names[1], result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "plt.rcParams['figure.figsize'] = 15, 8\n",
    "skplt.metrics.plot_confusion_matrix(y_test, predictions, normalize=True)\n",
    "plt.show()\n",
    "plt.savefig('CNN_SO_Confusion_Matrix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can just print the classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test loss={}, Test accuracy={}'.format(result[0],result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
